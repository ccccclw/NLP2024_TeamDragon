{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db119ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/guzhengwei/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "/Users/guzhengwei/anaconda3/envs/fml/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/Users/guzhengwei/anaconda3/envs/fml/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/Users/guzhengwei/anaconda3/envs/fml/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/Users/guzhengwei/anaconda3/envs/fml/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv('testset.csv')\n",
    "\n",
    "\n",
    "def calculate_sentence_bleu(reference, hypothesis):\n",
    "    \n",
    "    ref_tokens = word_tokenize(reference)\n",
    "    hyp_tokens = word_tokenize(hypothesis)\n",
    "    return sentence_bleu([ref_tokens], hyp_tokens, weights=(0.5, 0.5))\n",
    "\n",
    "df['BLEU_translation_correct'] = df.apply(lambda row: calculate_sentence_bleu(row['translation_correct'], row['Transformer_pred']), axis=1)\n",
    "df['BLEU_translation_wrong'] = df.apply(lambda row: calculate_sentence_bleu(row['translation_wrong'], row['Transformer_pred']), axis=1)\n",
    "df['BLEU_English_correct'] = df.apply(lambda row: calculate_sentence_bleu(row['English_correct'], row['Transformer_pred']), axis=1)\n",
    "df['BLEU_English_wrong'] = df.apply(lambda row: calculate_sentence_bleu(row['English_wrong'], row['Transformer_pred']), axis=1)\n",
    "\n",
    "df.to_csv('output_with_bleu_scores.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48fc2cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('output_with_bleu_scores.csv')\n",
    "\n",
    "# Compare BLEU scores and mark the presence of priming effect\n",
    "df['Priming_effect_translation'] = df['BLEU_translation_correct'] > df['BLEU_translation_wrong']\n",
    "df['Priming_effect_non_translation'] = df['BLEU_English_correct'] > df['BLEU_English_wrong']\n",
    "\n",
    "# Add columns for the priming effect result\n",
    "df['Translation_Priming_Result'] = df.apply(lambda x: 'Yes' if x['Priming_effect_translation'] else 'No', axis=1)\n",
    "df['Non_translation_Priming_Result'] = df.apply(lambda x: 'Yes' if x['Priming_effect_non_translation'] else 'No', axis=1)\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "output_file_path = 'output_with_priming_results.csv'\n",
    "df.to_csv(output_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4efddd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we'll need to read the CSV file into a pandas DataFrame.\n",
    "df = pd.read_csv('output_with_priming_results.csv')\n",
    "\n",
    "# Define the chunk size\n",
    "chunk_size = 30\n",
    "\n",
    "# Function to calculate the proportion of 'True' values within each chunk for a given column\n",
    "def calculate_chunk_proportion(column):\n",
    "    # Create an empty series to store the proportions\n",
    "    proportions = pd.Series(index=df.index, dtype=float)\n",
    "    \n",
    "    # Iterate over the DataFrame in steps of `chunk_size`\n",
    "    for start in range(0, len(df), chunk_size):\n",
    "        end = start + chunk_size\n",
    "        chunk = df[column][start:end]\n",
    "        proportion = chunk.sum() / chunk_size\n",
    "        proportions[start:end] = proportion\n",
    "    \n",
    "    return proportions\n",
    "\n",
    "# Calculate proportions for 'Priming_effect_translation' and 'Priming_effect_non_translation'\n",
    "df['Prime_Type_Translation'] = calculate_chunk_proportion('Priming_effect_translation')\n",
    "df['Prime_Type_Non_Translation'] = calculate_chunk_proportion('Priming_effect_non_translation')\n",
    "\n",
    "# Drop the original priming effect columns as they are not needed for the output file\n",
    "df.drop(columns=['Priming_effect_translation', 'Priming_effect_non_translation'], inplace=True)\n",
    "\n",
    "# Save the new DataFrame to a CSV file\n",
    "new_csv_path = 'priming_proportion_per_chunk.csv'\n",
    "df.to_csv(new_csv_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4945c129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU Scores for Translation Correct vs. Transformer_pred: {'Average BLEU-1': 0.8043645998576265, 'Average BLEU-2': 0.7030482166851667, 'Average BLEU-3': 0.5977987647484042, 'Average BLEU-4': 0.5277194839810125}\n",
      "Average BLEU Scores for Translation Wrong vs. Transformer_pred: {'Average BLEU-1': 0.6523497536538, 'Average BLEU-2': 0.42130098667302585, 'Average BLEU-3': 0.24803455393499932, 'Average BLEU-4': 0.1637474205303424}\n",
      "Average BLEU Scores for English Correct vs. Transformer_pred: {'Average BLEU-1': 0.43888146927882005, 'Average BLEU-2': 0.1897479025528721, 'Average BLEU-3': 0.0910041568887471, 'Average BLEU-4': 0.06338391289380382}\n",
      "Average BLEU Scores for English Wrong vs. Transformer_pred: {'Average BLEU-1': 0.3426083975340467, 'Average BLEU-2': 0.10702960145755797, 'Average BLEU-3': 0.05454627015362661, 'Average BLEU-4': 0.04061731343071909}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('testset.csv')\n",
    "\n",
    "# Function to calculate average BLEU scores for given n-grams\n",
    "def calculate_average_bleu(df, ref_column, hyp_column):\n",
    "    scores = {n: [] for n in range(1, 5)}\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        reference = [word_tokenize(row[ref_column].lower())]\n",
    "        hypothesis = word_tokenize(row[hyp_column].lower())\n",
    "        \n",
    "        for n in range(1, 5):\n",
    "            weight = tuple((1/n,) * n + (0,) * (4-n))\n",
    "            score = sentence_bleu(reference, hypothesis, weights=weight, smoothing_function=SmoothingFunction().method1)\n",
    "            scores[n].append(score)\n",
    "    \n",
    "    # Calculate the average score for each n-gram level\n",
    "    average_scores = {f'Average BLEU-{n}': sum(scores[n])/len(scores[n]) for n in scores}\n",
    "    return average_scores\n",
    "\n",
    "# Calculate average BLEU scores for each column pair\n",
    "avg_bleu_translation_correct = calculate_average_bleu(df, 'translation_correct', 'Transformer_pred')\n",
    "avg_bleu_translation_wrong = calculate_average_bleu(df, 'translation_wrong', 'Transformer_pred')\n",
    "avg_bleu_English_correct = calculate_average_bleu(df, 'English_correct', 'Transformer_pred')\n",
    "avg_bleu_English_wrong = calculate_average_bleu(df, 'English_wrong', 'Transformer_pred')\n",
    "\n",
    "# Print or store the results as needed\n",
    "print(\"Average BLEU Scores for Translation Correct vs. Transformer_pred:\", avg_bleu_translation_correct)\n",
    "print(\"Average BLEU Scores for Translation Wrong vs. Transformer_pred:\", avg_bleu_translation_wrong)\n",
    "print(\"Average BLEU Scores for English Correct vs. Transformer_pred:\", avg_bleu_English_correct)\n",
    "print(\"Average BLEU Scores for English Wrong vs. Transformer_pred:\", avg_bleu_English_wrong)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5246ab92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
